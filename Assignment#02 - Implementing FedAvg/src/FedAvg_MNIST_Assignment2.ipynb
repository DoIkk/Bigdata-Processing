{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJecgC6jPGdS"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds = 20\n",
    "num_clients = 100\n",
    "client_fraction = 0.01\n",
    "local_epochs = 1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFUad9s894ek"
   },
   "outputs": [],
   "source": [
    "def load_dataset_and_loaders():\n",
    "    # Define transformations to normalize the data\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    \n",
    "    # Load the MNIST dataset\n",
    "    train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    # Number of users\n",
    "    num_users = num_clients\n",
    "    \n",
    "    # Size of data per user\n",
    "    samples_per_user = len(train_data) // num_users \n",
    "    \n",
    "    # Create a dictionary to store the indices for each user\n",
    "    user_data = {i: [] for i in range(num_users)}\n",
    "    \n",
    "    # Shuffle the indices of the dataset to create a random split\n",
    "    indices = np.arange(len(train_data))\n",
    "    np.random.seed(10)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for i in range(num_users):\n",
    "        user_data[i] = indices[i * samples_per_user: (i + 1) * samples_per_user]\n",
    "    \n",
    "    # Create a DataLoader for each user\n",
    "    train_loaders = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    for i in range(num_users):\n",
    "        # Create a subset of the dataset for each user\n",
    "        user_subset = Subset(train_data, user_data[i])\n",
    "        # Create a DataLoader for each subset\n",
    "        user_loader = DataLoader(user_subset, batch_size=batch_size, shuffle=True)\n",
    "        train_loaders.append(user_loader)\n",
    "    \n",
    "    # Test DataLoader for evaluating the global model\n",
    "    test_loader = DataLoader(test_data, batch_size=1000, shuffle=False)\n",
    "\n",
    "    return train_loaders, test_loader, train_data, user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Non-IID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot class distributions for each user\n",
    "def plot_class_distribution(user_class_counts):\n",
    "    num_users = len(user_class_counts)\n",
    "    fig, axs = plt.subplots(num_users // 4, 4, figsize=(20, 10))  # Adjust subplot layout for readability\n",
    "\n",
    "    for i, (user, counts) in enumerate(user_class_counts.items()):\n",
    "        ax = axs[i // 4, i % 4]\n",
    "        ax.bar(range(10), counts)\n",
    "        ax.set_title(f'User {user}')\n",
    "        ax.set_xlabel('Class')\n",
    "        ax.set_ylabel('Number of Samples')\n",
    "        ax.set_xticks(range(10))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lU1D06pTrKMN",
    "outputId": "57d99cc9-3159-44a6-a40a-c230b433b50f"
   },
   "outputs": [],
   "source": [
    "# Function to calculate class distribution for each user\n",
    "def calculate_and_plot_class_distribution(data, user_data):\n",
    "    user_class_counts = defaultdict(lambda: np.zeros(10, dtype=int))  # Initialize counts for 10 classes\n",
    "\n",
    "    # Iterate over each user\n",
    "    for user, indices in user_data.items():\n",
    "        # Count the classes for each user's subset of indices\n",
    "        for idx in indices:\n",
    "            label = data.targets[idx].item()  # Get the label of the sample\n",
    "            user_class_counts[user][label] += 1\n",
    "\n",
    "    # Display the class distribution for each user\n",
    "    for user, counts in user_class_counts.items():\n",
    "        print(f\"User {user}: {counts}\")\n",
    "\n",
    "    plot_class_distribution(user_class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Our Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ukra7lactU5m"
   },
   "outputs": [],
   "source": [
    "# Define the CNN Model with 2 convolutional layers and 2 fully-connected layers\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate model\n",
    "model = SimpleCNN()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions for Local Training and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Itp59j10tWTL"
   },
   "outputs": [],
   "source": [
    "def train_local(model, train_loader, epochs=1, lr=0.01):\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the global model on test data\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XlCyd47htYf7"
   },
   "outputs": [],
   "source": [
    "# Function to average weights from selected clients\n",
    "def average_weights(selected_models):\n",
    "\n",
    "    return avg_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0Y3qTjftaDL"
   },
   "outputs": [],
   "source": [
    "# Federated training function with client fraction C and test accuracy measurement\n",
    "def federated_training(num_rounds, num_clients, client_fraction, local_epochs, train_loaders, test_loader, lr=0.001):\n",
    "    \n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ltLwUt0KtdmL",
    "outputId": "644e0c43-8329-413f-e9ed-0d6bd980af0a"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_loaders, test_loader, train_data, user_data = load_dataset_and_loaders()\n",
    "    calculate_and_plot_class_distribution(train_data, user_data)\n",
    "    global_model = federated_training(num_rounds, num_clients, client_fraction, local_epochs, train_loaders, test_loader, lr=learning_rate)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
